{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autosklearn.classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../Datasets/Breast Cancer/breast_cancer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['class'].replace({'Benign': 0, 'Malignant': 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dataset.iloc[:, 1:-1], dataset.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/interpret_final/lib/python3.7/site-packages/autosklearn/automl.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, task, X_test, y_test, feat_type, dataset_name, only_return_configuration_space, load_models, is_classification)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/interpret_final/lib/python3.7/site-packages/autosklearn/automl.py\u001b[0m in \u001b[0;36m_get_logger\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2355/3281679348.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mautoml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautosklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoSklearnClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_left_for_this_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mautoml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/interpret_final/lib/python3.7/site-packages/autosklearn/estimators.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_test, y_test, feat_type, dataset_name)\u001b[0m\n\u001b[1;32m   1452\u001b[0m             \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m             \u001b[0mfeat_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeat_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m             \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m         )\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/interpret_final/lib/python3.7/site-packages/autosklearn/estimators.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoml_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoml_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_automl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoml_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/interpret_final/lib/python3.7/site-packages/autosklearn/automl.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_test, y_test, feat_type, dataset_name, only_return_configuration_space, load_models)\u001b[0m\n\u001b[1;32m   2311\u001b[0m             \u001b[0monly_return_configuration_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0monly_return_configuration_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m             \u001b[0mload_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_models\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2313\u001b[0;31m             \u001b[0mis_classification\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2314\u001b[0m         )\n\u001b[1;32m   2315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/interpret_final/lib/python3.7/site-packages/autosklearn/automl.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, task, X_test, y_test, feat_type, dataset_name, only_return_configuration_space, load_models, is_classification)\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/interpret_final/lib/python3.7/site-packages/autosklearn/automl.py\u001b[0m in \u001b[0;36m_fit_cleanup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Closing the dask infrastructure\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished closing the dask infrastructure\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "automl = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=120)\n",
    "automl.fit(X_train.values, y_train)\n",
    "y_hat = automl.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9707602339181286"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./models/breast_cancer_automl.pkl', 'wb') as f:\n",
    "    pickle.dump(automl, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./models/breast_cancer_automl.pkl', 'rb') as f:\n",
    "    automl = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = X_train.columns.tolist()\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X_train.columns.tolist(), class_names=['Benign', 'Malignant'], categorical_features=categorical_features, discretize_continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_x = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_fn = lambda i: explainer.explain_instance(X_test.iloc[i], automl.predict_proba, num_features=len(X_test.columns))\n",
    "def exp_fn_blk(xtest, exp_fn):\n",
    "    exp1 = []\n",
    "    for i in tqdm.tqdm(range(len(xtest))):\n",
    "        exp = exp_fn(i)\n",
    "        exp1.append(exp.as_map()[exp.available_labels()[0]])\n",
    "    return np.array(exp1)\n",
    "exp_fn_wrap = lambda x: np.array(exp_fn_blk(x, exp_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [1:35:15<00:00, 33.43s/it]  \n",
      "100%|██████████| 171/171 [13:57<00:00,  4.90s/it]\n"
     ]
    }
   ],
   "source": [
    "exp1 = exp_fn_wrap(text_x)\n",
    "exp2 = exp_fn_wrap(text_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./explanations/breast_cancer1.npy', exp1)\n",
    "np.save('./explanations/breast_cancer2.npy', exp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_exp(exp, feature_num):\n",
    "    enc_exp = np.zeros((len(exp),feature_num))\n",
    "    for i in range(len(exp)):\n",
    "        for j in range(len(exp[i])):\n",
    "            enc_exp[i][int(exp[i,j,0])] = exp[i,j,1]\n",
    "    return enc_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krish/anaconda3/envs/interpret/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py:984: RuntimeWarning: Explicit initial center position passed: performing only one init in KMeans instead of n_init=10.\n",
      "  self._check_params(X)\n"
     ]
    }
   ],
   "source": [
    "i = metrics.calc_identity(exp1, exp2)\n",
    "s = metrics.calc_separability(exp1)\n",
    "enc1 = enc_exp(exp1, len(X_test.columns))\n",
    "sb = metrics.calc_stability(enc1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100.0, 0, 171), (0, 171, 29241, 0.0), (13, 171))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i, s, sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_norm = metrics.normalize_test(X_train, X_test)\n",
    "sim = metrics.calc_similarity(exp1, X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13655143207870782"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [23:02<00:00,  8.09s/it]\n"
     ]
    }
   ],
   "source": [
    "list_monotonicity = []\n",
    "list_non_sensitivity = []\n",
    "list_effective_complexity = []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(text_x))):\n",
    "    atr = exp1[i]\n",
    "    sorted_atr = [j for i,j in atr]\n",
    "    sorted_feat = [i for i,j in atr]\n",
    "    y = np.zeros(2, dtype=int)\n",
    "    np.put(y, y_test.iloc[i], 1)\n",
    "    example = metrics.FeatureAttribution(automl, text_x[i], y, sorted_atr)\n",
    "    list_monotonicity.append(example.monotonicity())\n",
    "    list_non_sensitivity.append(example.non_sensitivity())\n",
    "    list_effective_complexity.append(example.effective_complexity(sorted_feat, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.07936507936507937\n",
      "0.0\n",
      "5.444444444444445\n",
      "-0.28571428571428575\n",
      "0.0\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(list_monotonicity))\n",
    "print(np.mean(list_non_sensitivity))\n",
    "print(np.mean(list_effective_complexity))\n",
    "\n",
    "print(np.median(list_monotonicity))\n",
    "print(np.median(list_non_sensitivity))\n",
    "print(np.median(list_effective_complexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [51:18<00:00, 18.00s/it]\n"
     ]
    }
   ],
   "source": [
    "trust = metrics.calc_trust_score(automl, text_x, exp1, 3, X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7582846003898635"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krish/anaconda3/envs/interpret_final/lib/python3.7/site-packages/lime/submodular_pick.py:58: UserWarning: Requested sample size larger than\n",
      "                              size of input data. Using all data\n",
      "  size of input data. Using all data\"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1786.3035340309143 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krish/anaconda3/envs/interpret_final/lib/python3.7/site-packages/lime/submodular_pick.py:58: UserWarning: Requested sample size larger than\n",
      "                              size of input data. Using all data\n",
      "  size of input data. Using all data\"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2112.1505782604218 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from lime import submodular_pick\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "exp1 = submodular_pick.SubmodularPick(explainer, X_test.values, automl.predict_proba, sample_size=200, num_features=len(X_test.columns), num_exps_desired=5)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "exp2 = submodular_pick.SubmodularPick(explainer, X_test.values, automl.predict_proba, sample_size=200, num_features=len(X_test.columns), num_exps_desired=5)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_imp(sp_obj):\n",
    "    W_pick=pd.DataFrame([dict(this.as_list(this.available_labels()[0])) for this in sp_obj.sp_explanations]).fillna(0)\n",
    "    W_pick['prediction'] = [this.available_labels()[0] for this in sp_obj.sp_explanations]\n",
    "    W=pd.DataFrame([dict(this.as_list(this.available_labels()[0])) for this in sp_obj.explanations]).fillna(0)\n",
    "    W['prediction'] = [this.available_labels()[0] for this in sp_obj.explanations]\n",
    "    np.abs(W.drop(\"prediction\", axis=1)).mean(axis=0).sort_values(ascending=False).head(25).sort_values(ascending=True)\n",
    "    grped_coeff = W.groupby(\"prediction\").mean()\n",
    "    grped_coeff = grped_coeff.T\n",
    "    return grped_coeff[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp1 = get_feature_imp(exp1)\n",
    "feat_imp2 = get_feature_imp(exp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19126089,  0.14075325,  0.10965043,  0.10805145,  0.06186295,\n",
       "        0.04336912,  0.04933314,  0.02352072, -0.00259187,  0.        ,\n",
       "        0.        , -0.00309163,  0.        ,  0.        ,  0.        ,\n",
       "       -0.00421735, -0.0061348 , -0.00038934,  0.        , -0.0018796 ,\n",
       "        0.00490765, -0.00167678, -0.01523544, -0.00023784])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_imp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_identity(feat_imp1, feat_imp2):\n",
    "    sum = 0\n",
    "    for i in range(len(feat_imp1)):\n",
    "        if(feat_imp1[i] == feat_imp2[i]):\n",
    "            sum += 1\n",
    "    return sum/len(feat_imp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041666666666666664"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = global_identity(feat_imp1, feat_imp2)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_fi(feat_imp):\n",
    "    return np.abs(feat_imp) / np.sum(np.abs(feat_imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_feat_imp = normal_fi(feat_imp1 + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entropy Ratio\n",
    "Ser = np.sum(normal_feat_imp*np.log(normal_feat_imp))/np.log(1/len(normal_feat_imp))\n",
    "\n",
    "# Kullback-Leibler Divergence\n",
    "Skl = np.sum(normal_feat_imp*np.log(normal_feat_imp/(1/len(normal_feat_imp))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gini(pfi):\n",
    "    sum = 0\n",
    "    for i in range(len(pfi)):\n",
    "        sum_curr = 0\n",
    "        for j in range(len(pfi)):\n",
    "            sum_curr += np.abs(pfi[i]-pfi[j])\n",
    "        sum += sum_curr\n",
    "    \n",
    "    return sum/(2*len(pfi)**2)*(np.sum(pfi)/len(pfi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sg = calc_gini(normal_feat_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6645978931917282, 1.0659259502487988, 0.0012855335424959721)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ser, Skl, Sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_alpha_fi(normal_pfi, alpha):\n",
    "    j_inst = 0\n",
    "    sum = 0\n",
    "    for i in range(len(normal_pfi)-1, -1, -1):\n",
    "        sum += normal_pfi[i]\n",
    "        if sum<=alpha:\n",
    "            j_inst = i\n",
    "        else:\n",
    "            break\n",
    "    return 1- (j_inst/len(normal_pfi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583333333333334"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_alpha_fi(normal_feat_imp, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_imp_all(sp_obj):\n",
    "    W_pick=pd.DataFrame([dict(this.as_list(this.available_labels()[0])) for this in sp_obj.sp_explanations]).fillna(0)\n",
    "    W_pick['prediction'] = [this.available_labels()[0] for this in sp_obj.sp_explanations]\n",
    "    W=pd.DataFrame([dict(this.as_list(this.available_labels()[0])) for this in sp_obj.explanations]).fillna(0)\n",
    "    W['prediction'] = [this.available_labels()[0] for this in sp_obj.explanations]\n",
    "    np.abs(W.drop(\"prediction\", axis=1)).mean(axis=0).sort_values(ascending=False).head(25).sort_values(ascending=True)\n",
    "    grped_coeff = W.groupby(\"prediction\").mean()\n",
    "    grped_coeff = grped_coeff.T\n",
    "    return grped_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1_feat_imp, class2_feat_imp = get_feature_imp_all(exp1)[0].values, get_feature_imp_all(exp1)[1].values\n",
    "normal_class1_fi, normal_class2_fi = normal_fi(class1_feat_imp), normal_fi(class2_feat_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4867607285015266"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(normal_class1_fi - normal_class2_fi, ord=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ciu import determine_ciu\n",
    "import tqdm\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_exp(exp, feature_num):\n",
    "    enc_exp = np.zeros((len(exp),feature_num))\n",
    "    for i in range(len(exp)):\n",
    "        for j in range(len(exp[i])):\n",
    "            enc_exp[i][int(exp[i,j,0])] = exp[i,j,1]\n",
    "    return enc_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_list = X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_fn_blk(xtest):\n",
    "    exp1 = []\n",
    "    for i in tqdm.tqdm(range(len(xtest))):\n",
    "        exp = determine_ciu(X_test.iloc[i:i+1], automl.predict_proba, X_train.to_dict('list'), samples = 1000, prediction_index = 1)\n",
    "        exp_list = [[feat_list.index(i), exp.ci[i]] for i in exp.ci]\n",
    "        exp1.append(exp_list)\n",
    "    return np.array(exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/171 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [09:29<00:00,  3.33s/it]\n",
      "100%|██████████| 171/171 [08:42<00:00,  3.06s/it]\n"
     ]
    }
   ],
   "source": [
    "exp1 = exp_fn_blk(X_test)\n",
    "exp2 = exp_fn_blk(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./explanations/breast_cancer_ciu1.npy', exp1)\n",
    "np.save('./explanations/breast_cancer_ciu2.npy', exp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krish/anaconda3/envs/interpret/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py:984: RuntimeWarning: Explicit initial center position passed: performing only one init in KMeans instead of n_init=10.\n",
      "  self._check_params(X)\n"
     ]
    }
   ],
   "source": [
    "i = metrics.calc_identity(exp1, exp2)\n",
    "s = metrics.calc_separability(exp1)\n",
    "enc1 = enc_exp(exp1, len(feat_list))\n",
    "sb = metrics.calc_stability(enc1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100.0, 0, 171), (0, 171, 29241, 0.0), (12, 171))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i, s, sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_norm = metrics.normalize_test(X_train, X_test)\n",
    "sim = metrics.calc_similarity(exp1, X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28670192680016876"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/171 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [15:23<00:00,  5.40s/it]\n"
     ]
    }
   ],
   "source": [
    "list_monotonicity = []\n",
    "list_non_sensitivity = []\n",
    "list_effective_complexity = []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(X_test))):\n",
    "    atr = exp1[i]\n",
    "    sorted_atr = [j for i,j in atr]\n",
    "    sorted_feat = [i for i,j in atr]\n",
    "    y = np.zeros(2, dtype=int)\n",
    "    np.put(y, y_test.iloc[i], 1)\n",
    "    example = metrics.FeatureAttribution(automl, X_test.to_numpy()[i], y, sorted_atr)\n",
    "    list_monotonicity.append(example.monotonicity())\n",
    "    list_non_sensitivity.append(example.non_sensitivity())\n",
    "    list_effective_complexity.append(example.effective_complexity(sorted_feat, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3299916457811195\n",
      "0.0\n",
      "5.421052631578948\n",
      "0.3333333333333334\n",
      "0.0\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(list_monotonicity))\n",
    "print(np.mean(list_non_sensitivity))\n",
    "print(np.mean(list_effective_complexity))\n",
    "\n",
    "print(np.median(list_monotonicity))\n",
    "print(np.median(list_non_sensitivity))\n",
    "print(np.median(list_effective_complexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [48:53<00:00, 17.16s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7222222222222223"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.calc_trust_score(automl, X_test.to_numpy(), exp1, 3, X_train.columns.to_list())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RULEFIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrules import SkopeRules\n",
    "import metrics_rules\n",
    "import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SkopeRules(max_depth_duplication=2,\n",
    "                    n_estimators=512,\n",
    "                    precision_min=0.3,\n",
    "                    recall_min=0.1,\n",
    "                    feature_names=X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 16.41026282310486 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.058643341064453125 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "top_rules1 = clf.score_top_rules(X_test)\n",
    "top_rules2 = clf.score_top_rules(X_test)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 171, 171)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14336, 171, 29241, 49.02705105844533)\n",
      "(12, 171)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krish/anaconda3/envs/interpret/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py:984: RuntimeWarning: Explicit initial center position passed: performing only one init in KMeans instead of n_init=10.\n",
      "  self._check_params(X)\n"
     ]
    }
   ],
   "source": [
    "i = metrics_rules.calc_identity_rules(top_rules1, top_rules2)\n",
    "print(i)\n",
    "\n",
    "s = metrics_rules.calc_separability_rules(top_rules1)\n",
    "print(s)\n",
    "\n",
    "enc_rules = metrics_rules.exp_enc(clf, top_rules1)\n",
    "sb = metrics_rules.calc_stability_rules(enc_rules, y_test)\n",
    "print(sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20462933267421185\n"
     ]
    }
   ],
   "source": [
    "X_test_norm = metrics_rules.normalize_test(X_train, X_test)\n",
    "sim = metrics_rules.calc_similarity(enc_rules, X_test_norm)\n",
    "print(sim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RULEMATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = X_train.columns.tolist()\n",
    "continuous_features = X_train.columns.drop(categorical_features).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rulematrix\n",
    "import time\n",
    "import metrics_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_continuous = [True if i in continuous_features else False for i in X_train.columns.tolist()]\n",
    "is_categorical = [True if i in categorical_features else False for i in X_train.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate = rulematrix.surrogate.rule_surrogate(\n",
    "    automl.predict,\n",
    "    X_train,\n",
    "    sampling_rate=4,\n",
    "    is_continuous=is_continuous,\n",
    "    is_categorical=is_categorical,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = X_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_fn_blk(xtest):\n",
    "    exp1 = []\n",
    "    for i in range(len(xtest)):\n",
    "        queried_rules = np.arange(surrogate.student.n_rules)[surrogate.student.decision_path(test_x[i].reshape(1,-1)).reshape(-1)]\n",
    "        exp1.append(queried_rules[-1])\n",
    "    return np.array(exp1)\n",
    "exp_fn_wrap = lambda x: np.array(exp_fn_blk(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.05341458320617676 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "exp1 = exp_fn_blk(test_x)\n",
    "exp2 = exp_fn_blk(test_x)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_exp(exp, n_features):\n",
    "    enc = []\n",
    "    for i in range(exp.shape[0]):\n",
    "        new = np.zeros(n_features)\n",
    "        for j in surrogate.student.rule_list[exp[i]].clauses:\n",
    "            new[j.feature_idx] = 1\n",
    "        enc.append(new)\n",
    "    return np.array(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_exp = enc_exp(exp1, X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 171, 171)\n",
      "(10916, 171, 29241, 37.331144625696794)\n",
      "(27, 171)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krish/anaconda3/envs/interpret/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py:984: RuntimeWarning: Explicit initial center position passed: performing only one init in KMeans instead of n_init=10.\n",
      "  self._check_params(X)\n"
     ]
    }
   ],
   "source": [
    "i = metrics_rules.calc_identity_rules(exp1, exp2)\n",
    "print(i)\n",
    "\n",
    "s = metrics_rules.calc_separability_rules(exp1)\n",
    "print(s)\n",
    "\n",
    "sb = metrics_rules.calc_stability_rules(enc_exp, y_test)\n",
    "print(sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_norm = metrics_rules.normalize_test(X_train, X_test)\n",
    "sim = metrics_rules.calc_similarity(enc_exp, X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26994695108724004"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANCHOR Global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anchor import anchor_tabular\n",
    "import anchor_utils\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "    y_train.unique().tolist(),\n",
    "    X_train.columns.tolist(),\n",
    "    X_train.values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance using Anchor\n",
    "def calc_fi(X_test, model, explainer):\n",
    "    all_exps = []\n",
    "    for i in tqdm.tqdm(range(len(X_test))):\n",
    "        exp = explainer.explain_instance(X_test.values[i], model.predict, threshold=0.95)\n",
    "        all_exps.append(exp.exp_map)\n",
    "    fi = anchor_utils.greedy_pick_anchor(all_exps, X_test.values, k = len(X_test.columns))\n",
    "    return fi\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [2:06:50<00:00, 44.51s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.49122807017543857\n",
      "1 0.5555555555555556\n",
      "2 0.6140350877192983\n",
      "3 0.672514619883041\n",
      "4 0.7192982456140351\n",
      "5 0.7602339181286549\n",
      "6 0.783625730994152\n",
      "7 0.8070175438596491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [2:12:27<00:00, 46.48s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5321637426900585\n",
      "1 0.6081871345029239\n",
      "2 0.6666666666666666\n",
      "3 0.7251461988304093\n",
      "4 0.7777777777777778\n",
      "5 0.8011695906432749\n",
      "6 0.8245614035087719\n",
      "7 0.8421052631578947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "exp1 = calc_fi(X_test, automl, explainer)\n",
    "exp2 = calc_fi(X_test, automl, explainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_fi(feat_imp):\n",
    "    feat_imp = np.array(feat_imp) + 1e-9\n",
    "    return np.abs(feat_imp) / np.sum(np.abs(feat_imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_feat_imp1 = normal_fi(exp1)\n",
    "normal_feat_imp2 = normal_fi(exp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def global_identity(feat_imp1, feat_imp2):\n",
    "    sum = 0\n",
    "    for i in range(len(feat_imp1)):\n",
    "        if(feat_imp1[i] == feat_imp2[i]):\n",
    "            sum += 1\n",
    "    return sum/len(feat_imp1)\n",
    "\n",
    "i = global_identity(normal_feat_imp1, normal_feat_imp2)\n",
    "i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entropy Ratio\n",
    "Ser = np.sum(normal_feat_imp1*np.log(normal_feat_imp1))/np.log(1/len(normal_feat_imp1))\n",
    "\n",
    "# Kullback-Leibler Divergence\n",
    "Skl = np.sum(normal_feat_imp1*np.log(normal_feat_imp1/(1/len(normal_feat_imp1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gini(pfi):\n",
    "    sum = 0\n",
    "    for i in range(len(pfi)):\n",
    "        sum_curr = 0\n",
    "        for j in range(len(pfi)):\n",
    "            sum_curr += np.abs(pfi[i]-pfi[j])\n",
    "        sum += sum_curr\n",
    "    \n",
    "    return sum/(2*len(pfi)**2)*(np.sum(pfi)/len(pfi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sg = calc_gini(normal_feat_imp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7964290574913572, 0.42331387453138963, 0.0073242187498563874)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ser, Skl, Sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_alpha_fi(normal_pfi, alpha):\n",
    "    j_inst = 0\n",
    "    sum = 0\n",
    "    for i in range(len(normal_pfi)-1, -1, -1):\n",
    "        sum += normal_pfi[i]\n",
    "        if sum<=alpha:\n",
    "            j_inst = i\n",
    "        else:\n",
    "            break\n",
    "    return 1- (j_inst/len(normal_pfi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_alpha_fi(normal_feat_imp1, 0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interpret",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
